{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1073bbc4",
   "metadata": {},
   "source": [
    "# GenAI with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8e99c",
   "metadata": {},
   "source": [
    "###### [Article: TowardsDataScience](https://towardsdatascience.com/genai-with-python-rag-with-llm-complete-tutorial-c276dda6707b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2272a2",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "ebfd90ad",
   "metadata": {},
   "source": [
    "#conda install -c conda-forge poppler\n",
    "import pdf2image #1.17.0\n",
    "\n",
    "doc_img = pdf2image.convert_from_path(\"data/doc_nvidia.pdf\", dpi=300, poppler_path = r\"D:\\Programs\\poppler-24.08.0\\Library\\bin\")\n",
    "\n",
    "print(\"pages:\", len(doc_img))\n",
    "doc_img[35]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "doc_img[32]",
   "id": "cffcfe2898fc49e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5cfa311",
   "metadata": {},
   "source": [
    "import pytesseract #0.3.10\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'D:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "doc_txt = []\n",
    "for page in tqdm(doc_img):\n",
    "    text = pytesseract.image_to_string(page)\n",
    "    doc_txt.append(text)\n",
    "\n",
    "doc_txt[35]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf168e14",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "id": "807be32c",
   "metadata": {},
   "source": [
    "title_map = {\n",
    "    \"4-12\":\"Business\",\n",
    "    \"13-33\":\"Risk Factors\",\n",
    "    \"34-44\":\"Financials\",\n",
    "    \"45-46\":\"Directors\",\n",
    "    \"47-83\":\"Data\"\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0150894d",
   "metadata": {},
   "source": [
    "lst_docs, lst_ids, lst_metadata = [], [], []\n",
    "for n,page in enumerate(doc_txt):\n",
    "    try:\n",
    "        ## get title\n",
    "        title = [v for k,v in title_map.items() if n in range(int(k.split(\"-\")[0]), int(k.split(\"-\")[1])+1)][0]\n",
    "        ## clean page\n",
    "        page = page.replace(\"Table of Contents\",\"\")\n",
    "        ## get paragraph\n",
    "        for i,p in enumerate(page.split('\\n\\n')):\n",
    "            if len(p.strip())>5:\n",
    "                lst_docs.append(p.strip())\n",
    "                lst_ids.append(str(n)+\"_\"+str(i))\n",
    "                lst_metadata.append({\"title\":title})\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(len(lst_docs), \"=\", len(lst_ids), \"=\", len(lst_metadata))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a141d49e",
   "metadata": {},
   "source": [
    "for id,doc,meta in zip(lst_ids[375:378], lst_docs[375:378], lst_metadata[375:378]):\n",
    "    print(id, \"-\", meta, \"\\n\", doc, \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f07d802",
   "metadata": {},
   "source": [
    "### LLM Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2237d385",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### TEXT"
   ]
  },
  {
   "cell_type": "code",
   "id": "9fbd402d",
   "metadata": {},
   "source": [
    "import ollama #0.2.1\n",
    "\n",
    "def keyword_generator(p, top=3):\n",
    "    system = \"Your main objective is to condense the content of the document into a concise summary of no more than 3 keywords, capturing the main points and themes. Produce ONLY 3 keywords, DO NOT generate more text.\"\n",
    "    prompt = \"summarize the following paragraph in 3 short keywords separated by comma(,):\\n\"+p + \"\\n\"\n",
    "    res = ollama.generate(model=\"llama3.2\", system=system, prompt=prompt)[\"response\"]\n",
    "    #lst = [k.replace(\"\\n\",\" \").strip() for k in res.split(\",\")][:top]\n",
    "    #str = \", \".join(lst)\n",
    "    return res.replace(\"\\n\",\" \").strip()\n",
    "\n",
    "# p = '''Professional artists, architects and designers use NVIDIA partner products accelerated with our GPUs and software platform for a range of creative and design\n",
    "# use cases, such as creating visual effects in movies or designing buildings and products. In addition, generative Al is expanding the market for our workstation-\n",
    "# class GPUs, as more enterprise customers develop and deploy Al applications with their data on-premises.'''\n",
    "p = ''' Year Ended\n",
    "Jan 28, 2024 Jan 29, 2023 Change\n",
    "($ in millions, except per share data)\n",
    "Revenue $ 60,922 $ 26,974 Up 126%\n",
    "Gross margin 72.7 % 56.9 % Up 15.8 pts\n",
    "Operating expenses $ 11,329 $ 11,132 Up 2%\n",
    "Operating income $ 32,972 $ 4,224 Up 681%\n",
    "Net income $ 29,760 $ 4,368 Up 581%\n",
    "Net income per diluted share $ 11.93 $ 1.74 Up 586%'''\n",
    "# p = '''We specialize in markets where our computing platforms can provide tremendous acceleration for applications. These platforms incorporate processors, interconnects, \n",
    "# software, algorithms, systems, and services\n",
    "#  to deliver unique value. Our platforms address four large markets where our expertise is critical: Data Center, Gaming, Professional Visualization, and Automotive.'''\n",
    "\n",
    "print(keyword_generator(p))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83f7c96c",
   "metadata": {},
   "source": [
    "for i,doc in tqdm(enumerate(lst_docs)):\n",
    "    lst_metadata[i][\"keywords\"] = keyword_generator(doc)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5b45098",
   "metadata": {},
   "source": [
    "for id,doc,meta in zip(lst_ids[375:378], lst_docs[375:378], lst_metadata[375:378]):\n",
    "    print(id, \"-\", meta, \"\\n\", doc, \"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "598d7e94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "###### TABLE"
   ]
  },
  {
   "cell_type": "code",
   "id": "774cb6ef",
   "metadata": {},
   "source": [
    "table = lst_docs[376]\n",
    "print(\"Table:\\n\", table)\n",
    "\n",
    "prompt = f\"Summarize the following table: {table}\"\n",
    "res = ollama.generate(model=\"llama3.2\", prompt=prompt)[\"response\"]\n",
    "\n",
    "print(\"\\nSummary:\\n\", res)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b710a195",
   "metadata": {},
   "source": [
    "table = lst_docs[376]\n",
    "print(\"Table:\\n\", table)\n",
    "\n",
    "prompt = f\"Give a concise summary of the following table: {table}\"\n",
    "res = ollama.generate(model=\"phi3\", prompt=prompt)[\"response\"]\n",
    "\n",
    "print(\"\\nSummary:\\n\", res)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "084042bb",
   "metadata": {},
   "source": [
    "###### IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "id": "f704314c",
   "metadata": {},
   "source": [
    "from matplotlib import image, pyplot\n",
    "\n",
    "image_file = \"data/image.png\"\n",
    "\n",
    "pyplot.imshow(image.imread(image_file))\n",
    "pyplot.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96a6bee0",
   "metadata": {},
   "source": [
    "## Encode the image into a string\n",
    "import base64\n",
    "\n",
    "def encode_image(path):\n",
    "    with open(path, \"rb\") as file:\n",
    "        return base64.b64encode(file.read()).decode('utf-8')\n",
    "\n",
    "img = encode_image(image_file)\n",
    "img[:1000]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c917475a",
   "metadata": {},
   "source": [
    "prompt = \"describe the image\"\n",
    "res = ollama.generate(model=\"llama3.2\", prompt=prompt, images=[img])[\"response\"]\n",
    "print(res)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0f4df04e",
   "metadata": {},
   "source": [
    "## LLaVA\n",
    "prompt = \"describe the image\"\n",
    "res = ollama.generate(model=\"llava\", prompt=prompt, images=[img])[\"response\"]\n",
    "print(res)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc35e953",
   "metadata": {},
   "source": [
    "###### PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8d1a2de",
   "metadata": {},
   "source": [
    "image_file = \"data/plot.png\"\n",
    "\n",
    "pyplot.imshow(image.imread(image_file))\n",
    "pyplot.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "850cff27",
   "metadata": {},
   "source": [
    "img = encode_image(image_file)\n",
    "\n",
    "prompt = \"Describe the image in detail. Be specific about graphs, such as bar plots\"\n",
    "res = ollama.generate(model=\"llava\", prompt=prompt, images=[img])[\"response\"]\n",
    "print(res)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c851a312",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Database"
   ]
  },
  {
   "cell_type": "code",
   "id": "3dad52f6",
   "metadata": {},
   "source": [
    "import chromadb #0.5.0\n",
    "\n",
    "db = chromadb.PersistentClient()\n",
    "db.list_collections()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6630d14f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "collection_name = \"nvidia\"\n",
    "\n",
    "if collection_name in [c.name for c in db.list_collections()]:\n",
    "    db.delete_collection(collection_name)\n",
    "    print(\"--- deleted ---\")\n",
    "\n",
    "collection = db.get_or_create_collection(name=collection_name, \n",
    "                                         embedding_function=chromadb.utils.embedding_functions.DefaultEmbeddingFunction())\n",
    "\n",
    "collection.add(documents=lst_docs, ids=lst_ids, metadatas=lst_metadata, \n",
    "               images=None, embeddings=None)\n",
    "\n",
    "collection.peek(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33c0fccf",
   "metadata": {},
   "source": [
    "query = \"how much is the revenue?\"\n",
    "collection.query(query_texts=[query])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a52fe1e",
   "metadata": {},
   "source": [
    "res_db = collection.query(query_texts=[query])[\"documents\"][0][0:10]\n",
    "context = ' '.join(res_db).replace(\"\\n\", \" \")\n",
    "context"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "16e355c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LLM Chat"
   ]
  },
  {
   "cell_type": "code",
   "id": "bcb6615b",
   "metadata": {},
   "source": [
    "# Chat with Context only\n",
    "res = ollama.chat(model=\"llama3.2\", \n",
    "                  messages=[{\"role\":\"system\", \"content\":\"Give the most accurate answer using only \\\n",
    "                                                         the folling information: \\n\"+context},\n",
    "                            {\"role\":\"user\", \"content\":query}])\n",
    "print(res[\"message\"][\"content\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1fa7dd6c",
   "metadata": {},
   "source": [
    "# Chat with knowledge + Context\n",
    "res = ollama.chat(model=\"llama3.2\", \n",
    "                  messages=[{\"role\":\"system\", \"content\":\"Give the most accurate answer using your knowledge \\\n",
    "                                                         and the folling additional information: \\n\"+context},\n",
    "                            {\"role\":\"user\", \"content\":query}])\n",
    "print(res[\"message\"][\"content\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c250364e",
   "metadata": {},
   "source": [
    "# Chat Stream\n",
    "res = ollama.chat(model=\"llama3.2\", \n",
    "                  messages=[{\"role\":\"system\", \"content\":\"Give the most accurate answer using your knowledge \\\n",
    "                                                         and the folling additional information: \\n\"+context},\n",
    "                            {\"role\":\"user\", \"content\":query}],\n",
    "                  stream=True)\n",
    "for chunk in res:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
