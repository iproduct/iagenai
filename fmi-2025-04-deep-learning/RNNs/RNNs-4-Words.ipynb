{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:05.888499600Z",
     "start_time": "2025-12-17T14:49:05.837940300Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 177
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <small>\n",
    "Copyright (c) 2017-21 Andrew Glassner\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "</small>\n",
    "\n",
    "\n",
    "\n",
    "# Deep Learning: A Visual Approach\n",
    "## by Andrew Glassner, https://glassner.com\n",
    "### Order: https://nostarch.com/deep-learning-visual-approach\n",
    "### GitHub: https://github.com/blueberrymusic\n",
    "------\n",
    "\n",
    "### What's in this notebook\n",
    "\n",
    "This notebook is provided as a “behind-the-scenes” look at code used to make some of the figures in this chapter. It is cleaned up a bit from the original code that I hacked together, and is only lightly commented. I wrote the code to be easy to interpret and understand, even for those who are new to Python. I tried never to be clever or even more efficient at the cost of being harder to understand. The code is in Python3, using the versions of libraries as of April 2021. \n",
    "\n",
    "This notebook may contain additional code to create models and images not in the book. That material is included here to demonstrate additional techniques.\n",
    "\n",
    "Note that I've included the output cells in this saved notebook, but Jupyter doesn't save the variables or data that were used to generate them. To recreate any cell's output, evaluate all the cells from the start up to that cell. A convenient way to experiment is to first choose \"Restart & Run All\" from the Kernel menu, so that everything's been defined and is up to date. Then you can experiment using the variables, data, functions, and other stuff defined in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 19: RNNs - Notebook 4: Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras steps are a modified version of the character-based RNN at\n",
    "https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "A lot of the word extraction and tokenizing was freely adapted from\n",
    "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a bit more casual than most of the notebooks in this repo,\n",
    "as it's only meant to do a single specific thing. There's no organization\n",
    "into useful functions and subroutines - it's just one cell after another,\n",
    "computing things in sequence! Feel free to re-organize it if you'd like to \n",
    "do more general-purpose text generation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:06.018337800Z",
     "start_time": "2025-12-17T14:49:05.891510400Z"
    }
   },
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout, InputLayer\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "\n",
    "# The Natural Language Toolkit (NLTK) can be found and installed from\n",
    "# https://www.nltk.org/\n",
    "import nltk\n",
    "import nltk.data\n",
    "\n",
    "# Tell nltk which package we want it to use\n",
    "nltk.download('punkt')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\office27\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 178
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:06.160093600Z",
     "start_time": "2025-12-17T14:49:06.050059700Z"
    }
   },
   "source": [
    "# Workaround for Keras issues on Mac computers (you can comment this\n",
    "# out if you're not on a Mac, or not having problems)\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ],
   "outputs": [],
   "execution_count": 179
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:06.257948800Z",
     "start_time": "2025-12-17T14:49:06.192277900Z"
    }
   },
   "source": [
    "# Global parameters\n",
    "\n",
    "Vocabulary_size = 8000\n",
    "Batch_size = 64  # Set to 1 below if we're stateful\n",
    "Learning_rate = 0.01\n",
    "Num_epochs = 500\n",
    "Start_epoch = 1\n",
    "Source_text_file = 'input_data/holmes.txt'\n",
    "Window_size = 40\n",
    "Window_step = 3\n",
    "Generated_text_length = 600\n",
    "Random_seed = 42\n",
    "Cells_per_layer = [8, 8]\n",
    "Use_dropout = [True] * len(Cells_per_layer)\n",
    "Dropout_rate = [0.3] * len(Cells_per_layer)\n",
    "Stateful_model = True\n",
    "File_writer = None\n",
    "Model_name = 'Layers-' + str(Cells_per_layer) + '-stateful-' + str(Stateful_model)\n",
    "\n",
    "if Stateful_model:\n",
    "    Batch_size = 1  # so we can predict with just 1, probably better to modify predictions\n",
    "    Window_step = Window_size  # samples are sequential, not overlapping\n",
    "\n",
    "Unknown_token = \"GLORP\"  # all words not in vocabulary"
   ],
   "outputs": [],
   "execution_count": 180
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:06.384121400Z",
     "start_time": "2025-12-17T14:49:06.301715200Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 180
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:08.338908900Z",
     "start_time": "2025-12-17T14:49:06.416168800Z"
    }
   },
   "source": [
    "# read in text one sentence at a time: https://stackoverflow.com/questions/4576077/python-split-text-on-sentences\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "fp = open(Source_text_file, encoding='utf-8')\n",
    "data = fp.read()\n",
    "tokenized_sentences = tokenizer.tokenize(data)\n",
    "\n",
    "# remove punctuation https://stackoverflow.com/questions/23317458/how-to-remove-punctuation\n",
    "punctuations = [\n",
    "    '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*',\n",
    "    '+', ',', '-', '.', '/', ':', ';', '<', '=', '>',\n",
    "    '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}',\n",
    "    '~', \"''\", \"`\", \"\\\"\", \",\", \"-\", \"\\n\", \"\\r\", \"”\"\n",
    "]\n",
    "sentences = []\n",
    "for sentence in tokenized_sentences:\n",
    "    no_punc = \" \".join(\"\".join([\" \" + ch + \" \" if ch in punctuations else ch for ch in sentence]).split())\n",
    "    sentences.append(no_punc)\n",
    "\n",
    "print(\"found \", len(sentences), \" sentences\")\n",
    "\n",
    "# sentences is an array of strings. Each string is what the tokenizer decided made\n",
    "# up an English-language \"sentence\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found  16720  sentences\n"
     ]
    }
   ],
   "execution_count": 181
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:08.394964300Z",
     "start_time": "2025-12-17T14:49:08.359854600Z"
    }
   },
   "source": [
    "sentences[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE ADVENTURES OF SHERLOCK HOLMES by SIR ARTHUR CONAN DOYLE I .'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 182
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:08.494159700Z",
     "start_time": "2025-12-17T14:49:08.413224300Z"
    }
   },
   "source": [
    "text_as_words = []\n",
    "for s in sentences:\n",
    "    words = s.split()\n",
    "    for w in words:\n",
    "        text_as_words.append(w)\n",
    "print(\"the text contains \", len(text_as_words), \" words\")\n",
    "# allwords is all the words in the text after tokenizing and removing punctuation"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the text contains  366463  words\n"
     ]
    }
   ],
   "execution_count": 183
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:08.773613200Z",
     "start_time": "2025-12-17T14:49:08.509496700Z"
    }
   },
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(text_as_words)\n",
    "number_of_unique_tokens = 1 + len(word_freq.items())  # add 1 for the \"unknown_token\"\n",
    "\n",
    "# Get the most common words \n",
    "vocab = word_freq.most_common(Vocabulary_size - 1)\n",
    "print(\"Found \", len(vocab), \" distinct words\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  7999  distinct words\n"
     ]
    }
   ],
   "execution_count": 184
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:08.858120700Z",
     "start_time": "2025-12-17T14:49:08.781050700Z"
    }
   },
   "source": [
    "# build index_to_word and word_to_index dictionaries\n",
    "unique_words = [v[0] for v in vocab]\n",
    "unique_words.append(Unknown_token)\n",
    "unique_words = sorted(list(set(unique_words)))\n",
    "print('number of unique vocabulary words being used:', len(unique_words))\n",
    "word_to_index = dict((w, i) for i, w in enumerate(unique_words))\n",
    "index_to_word = dict((i, w) for i, w in enumerate(unique_words))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique vocabulary words being used: 8000\n"
     ]
    }
   ],
   "execution_count": 185
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T17:49:00.897991500Z",
     "start_time": "2025-12-17T17:48:59.924593300Z"
    }
   },
   "source": [
    "print('Using vocabulary size %d.' % Vocabulary_size)\n",
    "for i in range(100):\n",
    "    print(\"word popularity \" + str(i) + \": <\" + vocab[i][0] + \"> used \" + str(vocab[i][1]) + \" times\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vocabulary_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mUsing vocabulary size \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m'\u001B[39m % \u001B[43mVocabulary_size\u001B[49m)\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[32m100\u001B[39m):\n\u001B[32m      3\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mword popularity \u001B[39m\u001B[33m\"\u001B[39m + \u001B[38;5;28mstr\u001B[39m(i) + \u001B[33m\"\u001B[39m\u001B[33m: <\u001B[39m\u001B[33m\"\u001B[39m + vocab[i][\u001B[32m0\u001B[39m] + \u001B[33m\"\u001B[39m\u001B[33m> used \u001B[39m\u001B[33m\"\u001B[39m + \u001B[38;5;28mstr\u001B[39m(vocab[i][\u001B[32m1\u001B[39m]) + \u001B[33m\"\u001B[39m\u001B[33m times\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'Vocabulary_size' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:09.019256300Z",
     "start_time": "2025-12-17T14:49:08.934595600Z"
    }
   },
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i in range(len(text_as_words)):\n",
    "    if not text_as_words[i] in word_to_index:\n",
    "        text_as_words[i] = Unknown_token"
   ],
   "outputs": [],
   "execution_count": 187
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:09.086559300Z",
     "start_time": "2025-12-17T14:49:09.021252900Z"
    }
   },
   "source": [
    "# make huge list of windowed fragments\n",
    "fragments = []\n",
    "next_words = []\n",
    "for i in range(0, len(text_as_words) - Window_size, Window_step):\n",
    "    fragments.append(text_as_words[i: i + Window_size])\n",
    "    next_words.append(text_as_words[i + Window_size])\n",
    "print('number of fragments created:', len(fragments))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of fragments created: 9161\n"
     ]
    }
   ],
   "execution_count": 188
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:09.145139Z",
     "start_time": "2025-12-17T14:49:09.097344400Z"
    }
   },
   "source": [
    "# Clip the fragments so it's a multiple of the batch size\n",
    "keep_fragments = 64 * int(len(fragments) / 64.)\n",
    "fragments = fragments[0:keep_fragments]"
   ],
   "outputs": [],
   "execution_count": 189
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:10.197019800Z",
     "start_time": "2025-12-17T14:49:09.149142Z"
    }
   },
   "source": [
    "# Create the training data\n",
    "# X is a boolean array that is number-of-fragments * Window_size * vocabulary_size\n",
    "#    That is, every fragment contains Window_size entries, one for each word\n",
    "#    Each word is given by a one-hot encoding whose length is the total number of word tokens\n",
    "# y is a boolean array that is number-of-fragments * vocabulary_size\n",
    "#    Each entry is the one-hot encoding of the word that follows the corresponding fragment\n",
    "\n",
    "X = np.zeros((len(fragments), Window_size, Vocabulary_size), dtype=bool)\n",
    "y = np.zeros((len(fragments), Vocabulary_size), dtype=bool)\n",
    "for i, fragment in enumerate(fragments):\n",
    "    for t, word in enumerate(fragment):\n",
    "        X[i, t, word_to_index[word]] = 1\n",
    "    y[i, word_to_index[next_words[i]]] = 1\n",
    "print(\"Training data:\")\n",
    "print(\"   X.shape = \", X.shape)\n",
    "print(\"   y.shape = \", y.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "   X.shape =  (9152, 40, 8000)\n",
      "   y.shape =  (9152, 8000)\n"
     ]
    }
   ],
   "execution_count": 190
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:10.248921400Z",
     "start_time": "2025-12-17T14:49:10.223955400Z"
    }
   },
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    # layer 1 is special\n",
    "    if Stateful_model:\n",
    "        if Batch_size != 1:\n",
    "            print(\"*** WARNING! *** build_stateful_model: Batch_size should be 1\")\n",
    "        model.add(InputLayer(batch_input_shape=(1, Window_size, Vocabulary_size)))\n",
    "        model.add(LSTM(Cells_per_layer[0], return_sequences=len(Cells_per_layer) > 1,\n",
    "                       stateful=True))\n",
    "    else:\n",
    "        model.add(LSTM(Cells_per_layer[0], return_sequences=True,\n",
    "                       input_shape=(Window_size, Vocabulary_size)))\n",
    "    if Use_dropout[0]:\n",
    "        model.add(Dropout(Dropout_rate[0]))\n",
    "    for i in range(1, len(Cells_per_layer)):\n",
    "        return_sequence = i < len(Cells_per_layer) - 1\n",
    "        model.add(LSTM(Cells_per_layer[i], return_sequences=return_sequence))\n",
    "        if Use_dropout:\n",
    "            model.add(Dropout(Dropout_rate[i]))\n",
    "    model.add(Dense(Vocabulary_size))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #optimizer = RMSprop(lr=Learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 191
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:10.281476600Z",
     "start_time": "2025-12-17T14:49:10.252922600Z"
    }
   },
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = preds[0:len(word_to_index)]\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ],
   "outputs": [],
   "execution_count": 192
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:10.323705800Z",
     "start_time": "2025-12-17T14:49:10.297854200Z"
    }
   },
   "source": [
    "def print_string(out_str=''):\n",
    "    print(out_str, end='')\n",
    "    File_writer.write(out_str)"
   ],
   "outputs": [],
   "execution_count": 193
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:10.369500Z",
     "start_time": "2025-12-17T14:49:10.340602400Z"
    }
   },
   "source": [
    "def print_report():\n",
    "    print_string(\"Vocabulary_size = \" + str(Vocabulary_size) + \"\\n\")\n",
    "    print_string(\"Batch_size = \" + str(Batch_size) + \"\\n\")\n",
    "    print_string(\"Learning_rate = \" + str(Learning_rate) + \"\\n\")\n",
    "    print_string(\"Source_text_file = \" + str(Source_text_file) + \"\\n\")\n",
    "    print_string(\"Window_size = \" + str(Window_size) + \"\\n\")\n",
    "    print_string(\"Window_step = \" + str(Window_step) + \"\\n\")\n",
    "    print_string(\"Batch_size = \" + str(Batch_size) + \"\\n\")\n",
    "    print_string(\"Num_epochs = \" + str(Num_epochs) + \"\\n\")\n",
    "    print_string(\"Generated_text_length = \" + str(Generated_text_length) + \"\\n\\n\")\n",
    "\n",
    "    print_string(\"Input text file: \" + Source_text_file + '\\n')\n",
    "    print_string(\"    output file: \" + output_file + '\\n\\n')\n",
    "    print_string(\"full text: \" + str(len(sentences)) + \" sentences\\n\")\n",
    "    print_string(\"           \" + str(len(text_as_words)) + \" tokens\\n\\n\")\n",
    "    print_string(\"           \" + str(number_of_unique_tokens) + \" unique tokens in source\\n\")\n",
    "    print_string(\"           \" + str(len(unique_words)) + \" unique words (tokens) being used\\n\")\n",
    "    print_string('number of fragments created: ' + str(len(fragments)) + '\\n')\n",
    "    print_string('    resulting in ' + str(len(fragments) / 64.0) + ' batches\\n\\n')\n",
    "\n",
    "    print_string('Model_name: ' + Model_name + '\\n')\n",
    "    print_string('Stateful_model: ' + str(Stateful_model) + '\\n')\n",
    "    print_string('Cells per layer: ' + str(Cells_per_layer) + '\\n')\n",
    "    print_string('Use dropout: ' + str(Use_dropout) + '\\n')\n",
    "    print_string('Dropout rate: ' + str(Dropout_rate) + '\\n\\n')"
   ],
   "outputs": [],
   "execution_count": 194
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:49:10.590885200Z",
     "start_time": "2025-12-17T14:49:10.385688800Z"
    }
   },
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_17\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_17\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_13 (\u001B[38;5;33mLSTM\u001B[0m)                  │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m40\u001B[0m, \u001B[38;5;34m8\u001B[0m)             │       \u001B[38;5;34m256,288\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m40\u001B[0m, \u001B[38;5;34m8\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_14 (\u001B[38;5;33mLSTM\u001B[0m)                  │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m8\u001B[0m)                 │           \u001B[38;5;34m544\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m8\u001B[0m)                 │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m8000\u001B[0m)              │        \u001B[38;5;34m72,000\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (\u001B[38;5;33mActivation\u001B[0m)       │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m8000\u001B[0m)              │             \u001B[38;5;34m0\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">256,288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">72,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m328,832\u001B[0m (1.25 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">328,832</span> (1.25 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m328,832\u001B[0m (1.25 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">328,832</span> (1.25 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 195
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-17T15:01:48.671268700Z"
    }
   },
   "source": [
    "# train the model, output generated text after each iteration\n",
    "\n",
    "output_file = Source_text_file.replace('/', '-')\n",
    "output_file = output_file.replace('.txt', '')\n",
    "output_file = 'Outputs/OUTPUT-' + output_file + '.txt'\n",
    "print(\"output going to output file \" + output_file + \"\\n\\n\")\n",
    "\n",
    "File_writer = open(output_file, 'w')\n",
    "print_report()\n",
    "model = build_model()\n",
    "Start_epoch = 1\n",
    "\n",
    "#### IMPORT!\n",
    "#import keras\n",
    "#model = keras.models.load_model('Models/Layers-[8, 8]-stateful-False-epoch-119.h5')\n",
    "#Start_epoch = 120\n",
    "\n",
    "shuffle = not Stateful_model\n",
    "\n",
    "np.random.seed(Random_seed)\n",
    "history_list = []\n",
    "\n",
    "for iteration in range(Start_epoch, Num_epochs):\n",
    "    print_string('\\n')\n",
    "    print_string('----------------------------------------------------------------------\\n')\n",
    "    print_string('Iteration ' + str(iteration) + '\\n')\n",
    "    history = model.fit(X, y, Batch_size, epochs=1, shuffle=shuffle)\n",
    "    history_list.append(history)\n",
    "    print_string('Loss from iteration ' + str(iteration) + ' = ' + str(history.history['loss']) + '\\n')\n",
    "\n",
    "    savefile = 'Models/' + Model_name + '-epoch-' + str(iteration) + '.h5'\n",
    "    print(\"saving to \", savefile)\n",
    "    model.save(savefile)\n",
    "    start_index = random.randint(0, len(text_as_words) - Window_size - 1)\n",
    "\n",
    "    for diversity in np.linspace(.5, 2, 7):\n",
    "        #for diversity in [1]:\n",
    "        print_string('\\n')\n",
    "        print_string('----- diversity: ' + str(diversity) + '\\n')\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text_as_words[start_index: start_index + Window_size]\n",
    "        #print(\"just made sentence =\",sentence)\n",
    "        generated = ' '.join(sentence)\n",
    "        print_string('----- Generating with seed: \"' + generated + '\"\\n----\\n')\n",
    "        print_string(generated)\n",
    "\n",
    "        for i in range(Generated_text_length):\n",
    "            x = np.zeros((1, Window_size, Vocabulary_size))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x[0, t, word_to_index[word]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = index_to_word[next_index]\n",
    "\n",
    "            generated += ' ' + next_word\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "\n",
    "            print_string(' ' + next_word)\n",
    "\n",
    "        print_string('\\n')\n",
    "        File_writer.flush()\n",
    "File_writer.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output going to output file Outputs/OUTPUT-input_data-holmes.txt\n",
      "\n",
      "\n",
      "Vocabulary_size = 8000\n",
      "Batch_size = 1\n",
      "Learning_rate = 0.01\n",
      "Source_text_file = input_data/holmes.txt\n",
      "Window_size = 40\n",
      "Window_step = 40\n",
      "Batch_size = 1\n",
      "Num_epochs = 500\n",
      "Generated_text_length = 600\n",
      "\n",
      "Input text file: input_data/holmes.txt\n",
      "    output file: Outputs/OUTPUT-input_data-holmes.txt\n",
      "\n",
      "full text: 16720 sentences\n",
      "           366463 tokens\n",
      "\n",
      "           15099 unique tokens in source\n",
      "           8000 unique words (tokens) being used\n",
      "number of fragments created: 9152\n",
      "    resulting in 143.0 batches\n",
      "\n",
      "Model_name: Layers-[8, 8]-stateful-True\n",
      "Stateful_model: True\n",
      "Cells per layer: [8, 8]\n",
      "Use dropout: [True, True]\n",
      "Dropout rate: [0.3, 0.3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteration 1\n",
      "\u001B[1m9152/9152\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m207s\u001B[0m 22ms/step - loss: 6.7906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss from iteration 1 = [6.790600299835205]\n",
      "saving to  Models/Layers-[8, 8]-stateful-True-epoch-1.h5\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"hopes that it may induce you to speak frankly in return . There is complete confidence between my husband and me on all matters save one . That one is politics . On this his lips are sealed . He\"\n",
      "----\n",
      "hopes that it may induce you to speak frankly in return . There is complete confidence between my husband and me on all matters save one . That one is politics . On this his lips are sealed . He upon , my for and man she in I the his enough to a it her . of sound . was talking . . trace it you , , . . and I at , , could , and , and the , but and , the the But off to , , of , she the you , , he . was . . a you to as . who of his he the you the about . the me my , . attention . . a the between I said is ? , in . the to . his without but and in . , been the had as the was , hat the . my . the GLORP , up , upon could it with , with I the the the GLORP that GLORP me GLORP that the ? now I , and he you I . There - When GLORP the , . I , \" and , away , it have he I Ballarat only I to Holmes . I . have behind . . , , And , of GLORP of He ? a . a in . the GLORP , The GLORP GLORP us . ? . the , the the the is . but , - I GLORP but the you . , his to it a ? , or , of . GLORP possible , in , was of , he was GLORP . off who . were . , and to . my . ! , . GLORP GLORP of of upon are The but the ? , of of I . friend to to the and . , GLORP , , a a to , to . over one , , to down . this coffee , , he the , . , was , , . of . in the but . , . in the , , was have the away This . be ! , into , at the at , her , that that , our GLORP , . . with men , the , of , , for of I do The my The . , you I and the This GLORP , and . . at He had ! You , the , passed , a , . . by . an upon very could - be , , and , the to I , , without in up so to , , McCarthy an that the to room . , , you his and , . some . out , GLORP GLORP , the the , would the an . was , Then man asked upon . This . the put a that . GLORP the GLORP \" I my , GLORP the - them in . needs . he . . away Now a the , . , the the my , , . were . GLORP an mine in , . my a upon , a it . an , the . would GLORP for , a ? I ? , with . so the GLORP now it it . the , the me the , now to I , . he , that of into . , you , a be I There the . from , . , to was so come and his , couch to I and of . . was an . . well , , the , by the a one , GLORP . it I have . , am all I , on no , . the it it which , GLORP GLORP\n",
      "\n",
      "----- diversity: 0.75\n",
      "----- Generating with seed: \"hopes that it may induce you to speak frankly in return . There is complete confidence between my husband and me on all matters save one . That one is politics . On this his lips are sealed . He\"\n",
      "----\n",
      "hopes that it may induce you to speak frankly in return . There is complete confidence between my husband and me on all matters save one . That one is politics . On this his lips are sealed . He off on . when the to make clutched . I , . said everything , it front at had only the a . all , whole , too the than You . Major I you the after the the park . going my by the I , would GLORP of So at on"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
