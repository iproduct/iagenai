{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## <small>\n",
    "Copyright (c) 2017-21 Andrew Glassner\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "</small>\n",
    "\n",
    "\n",
    "\n",
    "# Deep Learning: A Visual Approach\n",
    "## by Andrew Glassner, https://glassner.com\n",
    "### Order: https://nostarch.com/deep-learning-visual-approach\n",
    "### GitHub: https://github.com/blueberrymusic\n",
    "------\n",
    "\n",
    "### What's in this notebook\n",
    "\n",
    "This notebook is provided to help you work with Keras and TensorFlow. It accompanies the bonus chapters for my book. The code is in Python3, using the versions of libraries as of April 2021.\n",
    "\n",
    "Note that I've included the output cells in this saved notebook, but Jupyter doesn't save the variables or data that were used to generate them. To recreate any cell's output, evaluate all the cells from the start up to that cell. A convenient way to experiment is to first choose \"Restart & Run All\" from the Kernel menu, so that everything's been defined and is up to date. Then you can experiment using the variables, data, functions, and other stuff defined in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Chapter 3 - Notebook 8: Generate text word by word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Keras steps are a modified version of the character-based RNN at\n",
    "https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "A lot of the word extraction and tokenizing was freely adapted from\n",
    "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "\n",
    "The Sherlock Holmes text is from Project Gutenberg\n",
    "https://www.gutenberg.org/"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:10.532073600Z",
     "start_time": "2025-12-17T15:04:10.474521500Z"
    }
   },
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import LSTM, Dropout, InputLayer\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import nltk.data\n",
    "import string"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:10.603721700Z",
     "start_time": "2025-12-17T15:04:10.542445300Z"
    }
   },
   "source": [
    "# Workaround for Keras issues on Mac computers (you can comment this\n",
    "# out if you're not on a Mac, or not having problems)\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:10.755004800Z",
     "start_time": "2025-12-17T15:04:10.689414200Z"
    }
   },
   "source": [
    "# Make a File_Helper for saving and loading files.\n",
    "\n",
    "save_files = True\n",
    "\n",
    "import os, sys, inspect\n",
    "current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "sys.path.insert(0, os.path.dirname(current_dir)) # path to parent dir\n",
    "from DLBasics_Utilities import File_Helper\n",
    "file_helper = File_Helper(save_files)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:10.950291800Z",
     "start_time": "2025-12-17T15:04:10.817579100Z"
    }
   },
   "source": [
    "# Get the stuff we need from the Natural Language Toolkit (NLTK)\n",
    "nltk.download('punkt')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\office27\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global parameters\n",
    "\n",
    "Of the following parameters,\n",
    "the most important is probably the number of epochs, `Num_epochs`.\n",
    "\n",
    "The more epochs you train for, the better the results. \n",
    "I've found that 500 is a good starting point,\n",
    "but depending on your computer, memory, and GPU (if you have one), that\n",
    "could take hours, or days, or even longer! \n",
    "On my late 2014 iMac (which has a GPU, but not\n",
    "one that TensorFlow can use), each epoch takes about 30 minutes,\n",
    "so 500 epochs would take a little more than 10 days!\n",
    "I ran that once a long time ago, but I'm not going to do it again now.\n",
    "\n",
    "Here I've set `Num_epochs` to 4 epochs just\n",
    "for demonstration purposes, \n",
    "but the output at that point isn't much to\n",
    "celebrate. You'll surely be able to crank that up if you\n",
    "have a more modern computer with a GPU,\n",
    "or you use a cloud service such as Colab (which \n",
    "offers free processing on their GPU-enabled systems)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:11.070438600Z",
     "start_time": "2025-12-17T15:04:11.002334300Z"
    }
   },
   "source": [
    "# Global parameters\n",
    "\n",
    "Vocabulary_size = 8000\n",
    "Batch_size = 64  # Set to 1 below if we're stateful\n",
    "Learning_rate = 0.01\n",
    "\n",
    "\n",
    "Num_epochs = 4\n",
    "Start_epoch = 1\n",
    "input_dir = file_helper.get_input_data_dir()\n",
    "Source_text_file = input_dir+'/holmes.txt'\n",
    "output_dir = file_helper.get_saved_output_dir()\n",
    "file_helper.check_for_directory(output_dir)\n",
    "Output_file = output_dir+'/generated-holmes.txt'\n",
    "\n",
    "Window_size = 40\n",
    "Window_step = 3\n",
    "Generated_text_length = 600\n",
    "Random_seed = 42\n",
    "Cells_per_layer = [8, 8]\n",
    "Use_dropout = [True] * len(Cells_per_layer)\n",
    "Dropout_rate = [0.3] * len(Cells_per_layer)\n",
    "Stateful_model = True  \n",
    "File_writer = None\n",
    "Model_name = 'Layers-'+str(Cells_per_layer)+'-stateful-'+str(Stateful_model)\n",
    "\n",
    "if Stateful_model:\n",
    "    Batch_size = 1             # so we can predict with just 1, probably better to modify predictions\n",
    "    Window_step = Window_size  # samples are sequential, not overlapping\n",
    "\n",
    "Unknown_token = \"GLORP\"  # all words not in vocabulary"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:13.320290400Z",
     "start_time": "2025-12-17T15:04:11.070438600Z"
    }
   },
   "source": [
    "# read in text one sentence at a time: https://stackoverflow.com/questions/4576077/python-split-text-on-sentences\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "fp = open(Source_text_file, encoding='utf-8')\n",
    "data = fp.read()\n",
    "tokenized_sentences = tokenizer.tokenize(data)\n",
    "\n",
    "# remove punctuation https://stackoverflow.com/questions/23317458/how-to-remove-punctuation\n",
    "punctuations = [\n",
    "    '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', \n",
    "    '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', \n",
    "    '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', \n",
    "    '~', \"''\",\"`\",\"\\\"\", \",\", \"-\", \"\\n\", \"\\r\", \"”\"\n",
    "    ]\n",
    "sentences = []\n",
    "for sentence in tokenized_sentences:\n",
    "    no_punc = \" \".join(\"\".join([\" \"+ch+\" \" if ch in punctuations else ch for ch in sentence]).split())\n",
    "    sentences.append(no_punc)\n",
    "    \n",
    "print(\"found \",len(sentences),\" sentences\")\n",
    "\n",
    "# sentences is an array of strings. Each string is what the tokenizer decided made\n",
    "# up an English-language \"sentence\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found  16720  sentences\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:13.452189700Z",
     "start_time": "2025-12-17T15:04:13.362056400Z"
    }
   },
   "source": [
    "text_as_words = []\n",
    "for s in sentences:\n",
    "    words = s.split()\n",
    "    for w in words:\n",
    "        text_as_words.append(w)\n",
    "print(\"the text contains \",len(text_as_words),\" words\")\n",
    "# text_as_words is all the words in the text after tokenizing and removing punctuation"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the text contains  366463  words\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:13.736988900Z",
     "start_time": "2025-12-17T15:04:13.468489200Z"
    }
   },
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(text_as_words)\n",
    "number_of_unique_tokens = 1 + len(word_freq.items())  # add 1 for the \"unknown_token\"\n",
    "\n",
    "# Get the most common words \n",
    "vocab = word_freq.most_common(Vocabulary_size-1)\n",
    "print(\"Found \",len(vocab),\" distinct words\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  7999  distinct words\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:13.833638200Z",
     "start_time": "2025-12-17T15:04:13.762466200Z"
    }
   },
   "source": [
    "# build index_to_word and word_to_index dictionaries\n",
    "unique_words = [v[0] for v in vocab]\n",
    "unique_words.append(Unknown_token)\n",
    "unique_words = sorted(list(set(unique_words)))\n",
    "print('number of unique vocabulary words being used:', len(unique_words))\n",
    "word_to_index = dict((w, i) for i, w in enumerate(unique_words))\n",
    "index_to_word = dict((i, w) for i, w in enumerate(unique_words))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique vocabulary words being used: 8000\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:13.897015600Z",
     "start_time": "2025-12-17T15:04:13.844151900Z"
    }
   },
   "source": [
    "print('Using vocabulary size %d.' % Vocabulary_size)\n",
    "for i in range(10):\n",
    "    print(\"word popularity \"+str(i)+\": <\"+vocab[i][0]+\"> used \"+str(vocab[i][1])+\" times\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 8000.\n",
      "word popularity 0: <,> used 22050 times\n",
      "word popularity 1: <.> used 18394 times\n",
      "word popularity 2: <the> used 15607 times\n",
      "word popularity 3: <and> used 7915 times\n",
      "word popularity 4: <of> used 7622 times\n",
      "word popularity 5: <I> used 7614 times\n",
      "word popularity 6: <to> used 7566 times\n",
      "word popularity 7: <a> used 7083 times\n",
      "word popularity 8: <that> used 5135 times\n",
      "word popularity 9: <\"> used 5093 times\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:13.996664Z",
     "start_time": "2025-12-17T15:04:13.898011800Z"
    }
   },
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i in range(len(text_as_words)):\n",
    "    if not text_as_words[i] in word_to_index:\n",
    "        text_as_words[i] = Unknown_token"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:14.075054300Z",
     "start_time": "2025-12-17T15:04:13.996664Z"
    }
   },
   "source": [
    "# make huge list of windowed fragments\n",
    "fragments = []\n",
    "next_words = []\n",
    "for i in range(0, len(text_as_words) - Window_size, Window_step):\n",
    "    fragments.append(text_as_words[i: i + Window_size])\n",
    "    next_words.append(text_as_words[i + Window_size])\n",
    "print('number of fragments created:', len(fragments))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of fragments created: 9161\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:14.108339700Z",
     "start_time": "2025-12-17T15:04:14.084052100Z"
    }
   },
   "source": [
    "# Clip the fragments so it's a multiple of the batch size\n",
    "keep_fragments = 64 * int(len(fragments)/64.)\n",
    "fragments = fragments[0:keep_fragments]"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:15.360918300Z",
     "start_time": "2025-12-17T15:04:14.131863700Z"
    }
   },
   "source": [
    "# Create the training data\n",
    "# X is a boolean array that is number-of-fragments * Window_size * vocabulary_size\n",
    "#    That is, every fragment contains Window_size entries, one for each word\n",
    "#    Each word is given by a one-hot encoding whose length is the total number of word tkens\n",
    "# y is a boolean array that is number-of-fragments * vocabulary_size\n",
    "#    Each entry is the one-hot encoding of the word that follows the corresponding fragment\n",
    "\n",
    "X = np.zeros((len(fragments), Window_size, Vocabulary_size), dtype=bool)\n",
    "y = np.zeros((len(fragments), Vocabulary_size), dtype=bool)\n",
    "for i, fragment in enumerate(fragments):\n",
    "    for t, word in enumerate(fragment):   \n",
    "        X[i, t, word_to_index[word]] = 1\n",
    "    y[i, word_to_index[next_words[i]]] = 1\n",
    "print(\"Training data:\")\n",
    "print(\"   X.shape = \",X.shape)\n",
    "print(\"   y.shape = \",y.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "   X.shape =  (9152, 40, 8000)\n",
      "   y.shape =  (9152, 8000)\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:15.447575800Z",
     "start_time": "2025-12-17T15:04:15.384544400Z"
    }
   },
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    # layer 1 is special\n",
    "    if Stateful_model:\n",
    "        if Batch_size != 1:\n",
    "            print(\"*** WARNING! *** build_stateful_model: Batch_size should be 1\")\n",
    "        model.add(InputLayer(batch_input_shape=(1, Window_size, Vocabulary_size)))\n",
    "        model.add(LSTM(Cells_per_layer[0], return_sequences=len(Cells_per_layer) > 1,\n",
    "                       stateful=True))\n",
    "    else:\n",
    "        model.add(LSTM(Cells_per_layer[0], return_sequences=True,\n",
    "                       input_shape=(Window_size, Vocabulary_size)))\n",
    "    if Use_dropout[0]:\n",
    "        model.add(Dropout(Dropout_rate[0]))\n",
    "    for i in range(1, len(Cells_per_layer)):\n",
    "        return_sequence = i<len(Cells_per_layer)-1\n",
    "        model.add(LSTM(Cells_per_layer[i], return_sequences=return_sequence))\n",
    "        if Use_dropout:\n",
    "            model.add(Dropout(Dropout_rate[i]))\n",
    "    model.add(Dense(Vocabulary_size))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    #optimizer = RMSprop(lr=Learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:15.487274700Z",
     "start_time": "2025-12-17T15:04:15.453743800Z"
    }
   },
   "source": [
    "# from http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = preds[0:len(word_to_index)]\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:15.542430300Z",
     "start_time": "2025-12-17T15:04:15.502806Z"
    }
   },
   "source": [
    "def print_string(out_str=''):\n",
    "    print(out_str, end='')\n",
    "    File_writer.write(out_str)"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:15.613616700Z",
     "start_time": "2025-12-17T15:04:15.545252300Z"
    }
   },
   "source": [
    "def print_report():\n",
    "    print_string(\"Vocabulary_size = \"+str(Vocabulary_size)+\"\\n\")\n",
    "    print_string(\"Batch_size = \"+str(Batch_size)+\"\\n\")\n",
    "    print_string(\"Learning_rate = \"+str(Learning_rate)+\"\\n\")\n",
    "    print_string(\"Source_text_file = \"+str(Source_text_file)+\"\\n\")\n",
    "    print_string(\"Window_size = \"+str(Window_size)+\"\\n\")\n",
    "    print_string(\"Window_step = \"+str(Window_step)+\"\\n\")\n",
    "    print_string(\"Batch_size = \"+str(Batch_size)+\"\\n\")\n",
    "    print_string(\"Num_epochs = \"+str(Num_epochs)+\"\\n\")\n",
    "    print_string(\"Generated_text_length = \"+str(Generated_text_length)+\"\\n\\n\")\n",
    "\n",
    "    print_string(\"Input text file: \"+Source_text_file+'\\n')\n",
    "    print_string(\"    output file: \"+Output_file+'\\n\\n')\n",
    "    print_string(\"full text: \"+str(len(sentences))+\" sentences\\n\")\n",
    "    print_string(\"           \"+str(len(text_as_words))+\" tokens\\n\\n\")\n",
    "    print_string(\"           \"+str(number_of_unique_tokens)+\" unique tokens in source\\n\")\n",
    "    print_string(\"           \"+str(len(unique_words))+\" unique words (tokens) being used\\n\")\n",
    "    print_string('number of fragments created: '+str(len(fragments))+'\\n')\n",
    "    print_string('    resulting in '+str(len(fragments)/64.0)+' batches\\n\\n')\n",
    "    \n",
    "    print_string('Model_name: '+Model_name+'\\n')\n",
    "    print_string('Stateful_model: '+str(Stateful_model)+'\\n')\n",
    "    print_string('Cells per layer: '+str(Cells_per_layer)+'\\n')\n",
    "    print_string('Use dropout: '+str(Use_dropout)+'\\n')\n",
    "    print_string('Dropout rate: '+str(Dropout_rate)+'\\n\\n')"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:04:16.013875800Z",
     "start_time": "2025-12-17T15:04:15.613616700Z"
    }
   },
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (\u001B[38;5;33mLSTM\u001B[0m)                   │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m40\u001B[0m, \u001B[38;5;34m8\u001B[0m)             │       \u001B[38;5;34m256,288\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001B[38;5;33mDropout\u001B[0m)               │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m40\u001B[0m, \u001B[38;5;34m8\u001B[0m)             │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001B[38;5;33mLSTM\u001B[0m)                   │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m8\u001B[0m)                 │           \u001B[38;5;34m544\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m8\u001B[0m)                 │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m8000\u001B[0m)              │        \u001B[38;5;34m72,000\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001B[38;5;33mActivation\u001B[0m)         │ (\u001B[38;5;34m1\u001B[0m, \u001B[38;5;34m8000\u001B[0m)              │             \u001B[38;5;34m0\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">256,288</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">72,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m328,832\u001B[0m (1.25 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">328,832</span> (1.25 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m328,832\u001B[0m (1.25 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">328,832</span> (1.25 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-17T15:04:16.016898Z"
    }
   },
   "source": [
    "# train the model, output generated text after each iteration\n",
    "# There needs to be a directory called \"Models\" in the same\n",
    "# directory as this file, or we'll get an error.\n",
    "\n",
    "File_writer = open(Output_file, 'w')\n",
    "print_report()\n",
    "model = build_model()\n",
    "Start_epoch = 1\n",
    "\n",
    "#### How to import from a saved model\n",
    "#import keras\n",
    "#model = keras.models.load_model('Models/Layers-[8, 8]-stateful-False-epoch-119.h5')\n",
    "#Start_epoch = 120\n",
    "\n",
    "shuffle = not Stateful_model\n",
    "\n",
    "np.random.seed(Random_seed)\n",
    "history_list = []\n",
    "\n",
    "for iteration in range(Start_epoch, Num_epochs):\n",
    "    print_string('\\n')\n",
    "    print_string('----------------------------------------------------------------------\\n')\n",
    "    print_string('Iteration '+str(iteration)+'\\n')\n",
    "    history = model.fit(X, y, Batch_size, epochs=1, shuffle=shuffle)  \n",
    "    history_list.append(history)\n",
    "    # if Stateful_model:\n",
    "    #     model.reset_states()\n",
    "    print_string('Loss from iteration '+str(iteration)+' = '+str(history.history['loss'])+'\\n')\n",
    "        \n",
    "    model_filename = Model_name+'-epoch-'+str(iteration)\n",
    "    print(\"saving model to file \",model_filename)\n",
    "    file_helper.save_model(model, model_filename)  \n",
    "    start_index = random.randint(0, len(text_as_words) - Window_size - 1)\n",
    "\n",
    "    for diversity in np.linspace(.5, 2, 7):\n",
    "    #for diversity in [1]:\n",
    "        print_string('\\n')\n",
    "        print_string('----- diversity: '+str(diversity)+'\\n')\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text_as_words[start_index: start_index + Window_size]\n",
    "        #print(\"just made sentence =\",sentence)\n",
    "        generated = ' '.join(sentence)\n",
    "        print_string('----- Generating with seed: \"' +generated+ '\"\\n----\\n')\n",
    "        print_string(generated)\n",
    "\n",
    "        for i in range(Generated_text_length):\n",
    "            x = np.zeros((1, Window_size, Vocabulary_size))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x[0, t, word_to_index[word]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]            \n",
    "            \n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = index_to_word[next_index]\n",
    "\n",
    "            generated += ' '+next_word\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "            \n",
    "            print_string(' '+next_word)\n",
    "\n",
    "        print_string('\\n')\n",
    "        File_writer.flush()\n",
    "File_writer.close()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary_size = 8000\n",
      "Batch_size = 1\n",
      "Learning_rate = 0.01\n",
      "Source_text_file = input_data/holmes.txt\n",
      "Window_size = 40\n",
      "Window_step = 40\n",
      "Batch_size = 1\n",
      "Num_epochs = 4\n",
      "Generated_text_length = 600\n",
      "\n",
      "Input text file: input_data/holmes.txt\n",
      "    output file: saved_output/generated-holmes.txt\n",
      "\n",
      "full text: 16720 sentences\n",
      "           366463 tokens\n",
      "\n",
      "           15099 unique tokens in source\n",
      "           8000 unique words (tokens) being used\n",
      "number of fragments created: 9152\n",
      "    resulting in 143.0 batches\n",
      "\n",
      "Model_name: Layers-[8, 8]-stateful-True\n",
      "Stateful_model: True\n",
      "Cells per layer: [8, 8]\n",
      "Use dropout: [True, True]\n",
      "Dropout rate: [0.3, 0.3]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Iteration 1\n",
      "\u001B[1m4814/9152\u001B[0m \u001B[32m━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━\u001B[0m \u001B[1m1:44\u001B[0m 24ms/step - loss: 7.2662"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
