{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMGCxuTtkgGrW/9WOsOQAIY",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/angelaaaateng/AIR_AI_Engineering_Course_2024/blob/main/Day1/1_Processing_IMDB_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Text Processing Short Activity\n",
    "\n",
    "**Instructions:**\n",
    "1. Download the IMDB Dataset from here or directly from our Github [here](https://github.com/angelaaaateng/AIR_AI_Engineering_Course_2024/raw/refs/heads/main/Datasets/IMDB_Dataset.csv)\n",
    "2. Tokenization with NLTK and SpaCy\n",
    "3. Stopword Removal\n",
    "4. Stemming & Lemmatization\n",
    "5. Compare Results. Once you're done, look at the differences between NLTK and spaCy. Which approach do you think is more suitable for this dataset? What are some differences that you saw in these 2 packages?\n",
    "\n"
   ],
   "metadata": {
    "id": "9BqHvDVEb6wd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ],
   "metadata": {
    "id": "7LH2qsg8Qsk5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4804707a-a5aa-4555-b31c-bbd0c81efb86"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the IMDB dataset from Github\n",
    "url = \"https://github.com/angelaaaateng/AIR_AI_Engineering_Course_2024/raw/refs/heads/main/Datasets/IMDB_Dataset.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows to understand the structure of the dataset\n",
    "data.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "s9yrsmL9cSUP",
    "outputId": "e67c7f77-bcf4-41af-dfe7-bc68c44c3fe9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Define functions for text preprocessing using NLTK\n",
    "def preprocess_nltk(text):\n",
    "    # Tokenization using NLTK\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords using NLTK's English stopword list\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Stemming using PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "    # Lemmatization using WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    return {'tokens': tokens, 'filtered_tokens': filtered_tokens, 'stemmed_tokens': stemmed_tokens, 'lemmatized_tokens': lemmatized_tokens}"
   ],
   "metadata": {
    "id": "m394DkPTbAHT"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Define functions for text preprocessing using SpaCy\n",
    "def preprocess_spacy(text, nlp):\n",
    "    # Tokenization using SpaCy's language model\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    # Remove stopwords using SpaCy's built-in stopword list\n",
    "    filtered_tokens = [token.text for token in doc if token.text.lower() not in spacy_stopwords]\n",
    "\n",
    "    # Lemmatization using SpaCy\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "    return {'tokens': tokens, 'filtered_tokens': filtered_tokens, 'lemmatized_tokens': lemmatized_tokens}"
   ],
   "metadata": {
    "id": "OPVfKzAbccNs"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load SpaCy's small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Preprocess the first review using NLTK\n",
    "nltk_preprocessed = preprocess_nltk(data['review'][0])\n",
    "print(\"NLTK Preprocessed Output:\\n\", nltk_preprocessed)\n",
    "\n",
    "# Preprocess the first review using SpaCy\n",
    "spacy_preprocessed = preprocess_spacy(data['review'][0], nlp)\n",
    "print(\"\\nSpaCy Preprocessed Output:\\n\", spacy_preprocessed)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sX82gyNccfhz",
    "outputId": "884141ff-e55c-4e81-da8b-6fcca11be3c8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Compare NLTK and SpaCy results\n",
    "print(\"\\nComparison of NLTK and SpaCy Tokenization:\")\n",
    "print(f\"NLTK Tokens: {nltk_preprocessed['filtered_tokens']}\")\n",
    "print(f\"SpaCy Tokens: {spacy_preprocessed['filtered_tokens']}\")\n",
    "# SpaCy’s tokenization might differ because it can handle contractions like \"isn't\" as \"is\" and \"n't\", while NLTK may treat them as a single token.\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Kl9Is_AcpMZ",
    "outputId": "c0491813-ff2c-46c4-eb9a-e7908fa3437b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "print(\"\\nComparison of NLTK and SpaCy Lemmatization:\")\n",
    "print(f\"NLTK Lemmatized Tokens: {nltk_preprocessed['lemmatized_tokens']}\")\n",
    "print(f\"SpaCy Lemmatized Tokens: {spacy_preprocessed['lemmatized_tokens']}\")\n",
    "# SpaCy’s lemmatization tends to be more accurate and context-aware because it uses a more comprehensive rule-based approach and vocabulary. NLTK’s WordNetLemmatizer, in contrast, sometimes only reduces words to their base form without considering context.\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_-NSxQrczIK",
    "outputId": "44d14c86-e74d-4e22-8347-b4cdca974d27"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Visual comparison of tokenization differences\n",
    "nltk_tokens = nltk_preprocessed['filtered_tokens']\n",
    "spacy_tokens = spacy_preprocessed['filtered_tokens']\n",
    "\n",
    "# Plotting token frequency for comparison\n",
    "plt.figure(figsize=(14, 6))\n",
    "nltk_token_counts = pd.Series(nltk_tokens).value_counts().head(10)\n",
    "spacy_token_counts = pd.Series(spacy_tokens).value_counts().head(10)\n",
    "\n",
    "# Plot NLTK Token Frequencies\n",
    "plt.subplot(1, 2, 1)\n",
    "nltk_token_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 10 NLTK Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot SpaCy Token Frequencies\n",
    "plt.subplot(1, 2, 2)\n",
    "spacy_token_counts.plot(kind='bar', color='orange')\n",
    "plt.title('Top 10 SpaCy Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Comparison of NLTK vs SpaCy Tokenization')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "1YXjyZnec5co",
    "outputId": "f49d2b15-2206-4513-b567-cbebf7293db1"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "TlR_6jg7dCC0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Differences Between NLTK and spaCy:\n",
    "1. **Ease of Use**:\n",
    "   - **NLTK**: Offers more control and is modular, but can be more complex and requires manual setup for many tasks.\n",
    "   - **spaCy**: Designed for fast, efficient NLP with pre-trained models, making it easier for practical applications.\n",
    "2. **Performance**:\n",
    "   - **NLTK**: Slower, especially for large datasets.\n",
    "   - **spaCy**: Optimized for performance, great for handling large text corpora quickly.\n",
    "3. **Applications**:\n",
    "   - **NLTK**: Ideal for research and experimentation, as it provides a wide range of tools and algorithms.\n",
    "   - **spaCy**: Better suited for production-level applications where speed and ease of deployment are critical.\n",
    "In short:\n",
    "- Use **NLTK** for flexibility and in-depth experimentation.\n",
    "- Use **spaCy** when you need speed and ready-to-use NLP pipelines for production systems.\n",
    "\n"
   ],
   "metadata": {
    "id": "zr0KjyPqiU6J"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "X-KIgT3qiWaK"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
